# Os kÃ¼tÃ¼phanesini iÃ§e aktarÄ±yoruz.
# Os kÃ¼tÃ¼phanesi, dosya ve dizin iÅŸlemleri iÃ§in gereklidir.
import os
# JSON verilerini okumak iÃ§in json kÃ¼tÃ¼phanesini iÃ§e aktarÄ±yoruz.
import json
# LangChain bileÅŸenlerini iÃ§e aktarÄ±yoruz.
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
# Ä°lerleme Ã§ubuÄŸu iÃ§in tqdm kÃ¼tÃ¼phanesini kullanÄ±yoruz.
from tqdm import tqdm

# Belgeleri parÃ§alara ayÄ±rmak ve vektÃ¶rleÅŸtirmek iÃ§in gerekli bileÅŸenleri ayarlÄ±yoruz.
# - RecursiveCharacterTextSplitter: metinleri kÃ¼Ã§Ã¼k parÃ§alara (chunk) bÃ¶lerek
#   embedding oluÅŸtururken daha iyi sonuÃ§ alÄ±nmasÄ±nÄ± saÄŸlar.
#   chunk_size: her parÃ§a iÃ§in hedef boyut (token bazlÄ± tahmini).
#   chunk_overlap: ardÄ±ÅŸÄ±k parÃ§alar arasÄ±nda Ã¶rtÃ¼ÅŸme miktarÄ±, baÄŸlam kaybÄ±nÄ± azaltÄ±r.
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=800,
    chunk_overlap=150
)

# TÃ¼rkÃ§e iÃ§in eÄŸitilmiÅŸ bir BERT tabanlÄ± embedding modeli kullanÄ±yoruz.
# model_kwargs ile cihaz belirleniyor (Ã¶r. 'cpu' veya 'cuda').
# encode_kwargs ile embeddinglerin normalize edilmesini isteyebiliriz.
embedding = HuggingFaceEmbeddings(
    model_name="emrecan/bert-base-turkish-cased-mean-nli-stsb-tr",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

# Chroma veritabanÄ±nÄ±n kaydedileceÄŸi dizin
db_path = "./.chroma/movie"

# EÄŸer veritabanÄ± dizini yoksa veya boÅŸsa, veritabanÄ±nÄ± oluÅŸturuyoruz.
if not os.path.exists(db_path) or not os.listdir(db_path):
    # TÃ¼m film verilerini JSON'dan okuyoruz (UTF-8 encoding ile)
    with open("all_movies_reviews.json", "r", encoding="utf-8") as f:
        movies_data = json.load(f)

    movie_docs = []
    # Her film iÃ§in aÃ§Ä±klama (desc) ve kullanÄ±cÄ± yorumlarÄ±nÄ± ayrÄ± Document'lar olarak hazÄ±rlÄ±yoruz.
    for movie in movies_data:
        # Metadata'da filmin adÄ±, tÃ¼rÃ¼, yÃ¶netmeni, puanÄ± ve url'si gibi alanlarÄ± saklÄ±yoruz.
        meta = {
            "name": movie.get("name"),
            "genre": ", ".join(movie.get("genre", [])),
            "directors": movie.get("directors"),
            "rating": movie.get("rating", {}).get("totalRating"),
            "url": movie.get("url")
        }

        # Filmin genel aÃ§Ä±klamasÄ± varsa, Document olarak ekle
        if movie.get("desc"):
            movie_docs.append(Document(
                page_content=movie["desc"],
                metadata={**meta, "type": "desc"}
            ))

        # Her bir kullanÄ±cÄ± yorumunu da ayrÄ± bir Document yapÄ±yoruz.
        # metadata iÃ§inde yorumun tipini ve kullanÄ±cÄ± puanÄ±nÄ± ekliyoruz.
        for rev in movie.get("reviews", []):
            if rev.get("review"):
                movie_docs.append(Document(
                    page_content=rev["review"],
                    metadata={**meta, "type": "review", "user_rating": rev.get("rating")}
                ))

    # OluÅŸturduÄŸumuz Document listelerini belirtilen splitter ile parÃ§alara ayÄ±rÄ±yoruz.
    splits = text_splitter.split_documents(movie_docs)

    # Chroma vektÃ¶r veritabanÄ±nÄ± baÅŸlatÄ±yoruz (daha sonra persist_directory iÃ§ine kaydedilecek)
    db = Chroma(
        collection_name="movie-db",
        embedding_function=embedding,
        persist_directory=db_path
    )

    # Ã‡ok bÃ¼yÃ¼k koleksiyonlarda bellek/sÃ¼re yÃ¶netimi iÃ§in batch ile ekleme yapÄ±yoruz.
    # batch_size'a dikkat: Ã§ok bÃ¼yÃ¼kse bellek tÃ¼ketimi artar, Ã§ok kÃ¼Ã§Ã¼kse yavaÅŸ olur.
    batch_size = 5000
    for i in tqdm(range(0, len(splits), batch_size), desc="ğŸ”¹ Belgeler ekleniyor"):
        batch = splits[i:i + batch_size]
        db.add_documents(batch)

    # Ä°ndekslenmiÅŸ veritabanÄ±nÄ± diske kaydediyoruz.
    db.persist()
    print("ğŸ‰ Film vektÃ¶r veritabanÄ± oluÅŸturuldu ve kaydedildi.")
else:
    # EÄŸer daha Ã¶nce oluÅŸturulmuÅŸsa yeniden oluÅŸturmayÄ±z â€” bu, geliÅŸtirme sÃ¼recinde zaman kazandÄ±rÄ±r.
    print("âœ… Film veritabanÄ± zaten mevcut, yeniden oluÅŸturulmadÄ±.")

# Projeyi Ã§alÄ±ÅŸtÄ±rÄ±rken kolayca kullanabilmek iÃ§in bir retriever Ã¶rneÄŸi oluÅŸturuyoruz.
# as_retriever ile dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼mÃ¼z nesne aramalarda (k-NN benzeri) kullanÄ±lacak.
# search_type: 'similarity' (kullanÄ±lan embeddding mesafe metriÄŸine gÃ¶re benzerlik arar)
# search_kwargs: k = dÃ¶ndÃ¼rÃ¼lecek benzer parÃ§a sayÄ±sÄ±
movie_retriever = Chroma(
    collection_name="movie-db",
    persist_directory=db_path,
    embedding_function=embedding
).as_retriever(
    search_type="similarity",
    search_kwargs={"k": 6}
)
